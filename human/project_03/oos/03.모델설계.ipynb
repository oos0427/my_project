{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 : 스마트폰, 이마, 색소침착, 전체데이터<br>\n",
    "내  용 : 모든라벨 (0 - 5) 6개에 대한 파이토치를 이용한 기본 CNN모델 테스트<br>\n",
    "결  과 : 한번호  1등급으로 모두 찍는 경향 확인<br>\n",
    "보  완 : 04. Feasibility of Learning 파일 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 완료!!\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비 1. 핸드폰, 전면, 이마, 메타데이터 전체\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = \"Data/Train_Data_Sets_02.pkl\"\n",
    "with open(path, \"rb\") as pickle_file:\n",
    "    train_data_sets = pickle.load(pickle_file)\n",
    "    print(\"데이터 로드 완료!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for idx, file_name in enumerate(train_data_sets[\"Metadata\"]): \n",
    "    pig = train_data_sets[\"Metadata\"][file_name]['annotations']['forehead_pigmentation']\n",
    "    # 올바른 논리 연산\n",
    "    if pig == 2:\n",
    "        keys.append(file_name)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 완료!!\n"
     ]
    }
   ],
   "source": [
    "path = \"Data/Val_Data_Sets_02.pkl\"\n",
    "with open(path, \"rb\") as pickle_file:\n",
    "    test_data_sets = pickle.load(pickle_file)\n",
    "    print(\"데이터 로드 완료!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #데이터 구성 확인\n",
    "# print(train_data_sets.keys())\n",
    "# print(train_data_sets['Images'].keys())\n",
    "# print(train_data_sets['Metadata'].keys())\n",
    "# print(train_data_sets['Images']['0002_03_F_01'])\n",
    "# print(train_data_sets['Metadata']['0002_03_F_01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비 1.\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_sets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_sets (dict): {'Images': dict, 'Metadata': dict}\n",
    "        \"\"\"\n",
    "        self.images = data_sets['Images']\n",
    "        self.metadata = data_sets['Metadata']\n",
    "        self.keys = list(self.images.keys())  # 공통 키 목록\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        image = self.images[key]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        target = torch.tensor(self.metadata[key]['annotations']['forehead_pigmentation'], dtype=torch.float32)  # 타겟 텐서\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 생성\n",
    "dataset = CustomDataset(train_data_sets)\n",
    "val_dataset = CustomDataset(test_data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 0:\n",
      "이미지 텐서 크기: torch.Size([3, 224, 224])\n",
      "타겟 값: tensor(1.)\n",
      "샘플 1:\n",
      "이미지 텐서 크기: torch.Size([3, 224, 224])\n",
      "타겟 값: tensor(1.)\n",
      "샘플 2:\n",
      "이미지 텐서 크기: torch.Size([3, 224, 224])\n",
      "타겟 값: tensor(1.)\n",
      "샘플 3:\n",
      "이미지 텐서 크기: torch.Size([3, 224, 224])\n",
      "타겟 값: tensor(1.)\n",
      "샘플 4:\n",
      "이미지 텐서 크기: torch.Size([3, 224, 224])\n",
      "타겟 값: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 첫 5개의 샘플 확인\n",
    "for idx, (image, target) in enumerate(dataset):\n",
    "    if idx < 5:\n",
    "        print(f\"샘플 {idx}:\")\n",
    "        print(\"이미지 텐서 크기:\", image.shape)\n",
    "        print(\"타겟 값:\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "    print(X.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602112"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "56*56*192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=24, out_channels=192, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(56*56*192, 1024)\n",
    "        # self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 56*56*192)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkinNet(\n",
      "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(24, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=602112, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc5): Linear(in_features=256, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SkinNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    loss_total = 0\n",
    "    correct_total = 0\n",
    "    model.train()\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 누적 손실 및 정확도 계산\n",
    "        loss_total += loss.item()\n",
    "        predicted_values = output.max(1, keepdim=True)[1]\n",
    "        correct = predicted_values.eq(labels.view_as(predicted_values)).sum().item()\n",
    "        correct_total += correct\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Batch : {idx}, Loss : {loss.item()}\")\n",
    "\n",
    "        # 에폭 종료 후 평균 손실 및 정확도 계산\n",
    "    loss_total /= len(train_loader)  # 배치 개수로 나누어 평균 계산\n",
    "    accuracy = correct_total / len(train_loader.dataset)  # 전체 데이터셋에서 정확도 계산\n",
    "    \n",
    "    # 마지막 배치 후 결과 출력\n",
    "    print(f\"Epoch Finished - Loss: {loss_total:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return loss_total, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    loss_total = 0\n",
    "    correct_total = 0\n",
    "    model.eval() # evaluation mode로 설정 -> batch-normalization, drop-out 수행 중지\n",
    "    with torch.no_grad(): # 가중치 업데이트 수행 중지\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels).item()\n",
    "            loss_total += loss\n",
    "            predicted_values = output.max(1, keepdim=True)[1]\n",
    "            correct = predicted_values.eq(labels.view_as(predicted_values)).sum().item()\n",
    "            correct_total += correct\n",
    "\n",
    "    loss_total /= ( len(val_dataset) / 4 )\n",
    "    accuracy = correct_total / len(val_dataset)\n",
    "\n",
    "    return loss_total, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행\n",
    "\n",
    "for epoch in range(10):    \n",
    "    train(model, train_loader, optimizer)\n",
    "    val_loss, val_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Epoch : {epoch + 1}, val_Loss: {val_loss}, val_Accuracy : {val_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oos-dl-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
